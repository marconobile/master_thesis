{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from model import get_generator\n",
    "from supervised_tools.create_train_val_data import create_train_val_dataloaders\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import numpy as np\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as Fqb\n",
    "from rdkit import Chem\n",
    "from utils.setup import setup\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch_lr_finder import LRFinder\n",
    "from utils.data_utils import mols_from_file, get_atoms_info, rdkit2pyg, pyg2rdkit, save_smiles\n",
    "from mappings import *\n",
    "\n",
    "import mappings\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.]], requires_grad=True) tensor([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5514, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([[0,1,0.]], requires_grad=True)\n",
    "target = torch.tensor([1])#torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(input, target)\n",
    "output = loss(input, target)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.]]) tensor([[0.0944, 0.1438, 0.1025, 0.4711, 0.1882]])\n",
      "torch.Size([1, 5]) torch.Size([1, 5])\n",
      "tensor(1.8104)\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.zeros(1, 5)\n",
    "input[0,0]=1\n",
    "target = torch.randn(1, 5).softmax(dim=1)\n",
    "print(input, target)\n",
    "print(input.shape, target.shape)\n",
    "\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guacm_smiles = \"/home/nobilm@usi.ch/master_thesis/guacamol/guacamol_v1_train.smiles\"\n",
    "train_guac_mols = mols_from_file(guacm_smiles, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1273104"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = rdkit2pyg(train_guac_mols) \n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n = 0\n",
    "for g in train_data: \n",
    "    if g.x.shape[0] > max_n:\n",
    "        max_n = g.x.shape[0]\n",
    "\n",
    "print(\"max_n \", max_n)\n",
    "mappings.max_num_node = max_n\n",
    "mappings.max_prev_node = mappings.max_num_node - 1\n",
    "# valid_data = train_data\n",
    "atom2num, num2atom, max_num_node = get_atoms_info(train_guac_mols)\n",
    "#!-------------------------------------------\n",
    "atom2num, num2atom, max_num_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "def get_atoms_info(mols):\n",
    "    atoms = set()\n",
    "    max_num = 0\n",
    "    for num_mol, m in enumerate(mols):\n",
    "        if m.GetNumAtoms() > max_num: max_num = m.GetNumAtoms()\n",
    "\n",
    "        atom_types = [atom.GetSymbol() for atom in m.GetAtoms()]\n",
    "        c.update(atom_types)\n",
    "        \n",
    "        for atom in atom_types:\n",
    "            atoms.add(atom)\n",
    "        \n",
    "    atom2num = {}\n",
    "    for i, atomType in enumerate(atoms):\n",
    "        atom2num[str(atomType)] = i\n",
    "\n",
    "    num2atom = {v:k for k,v in atom2num.items()}\n",
    "    print(\"TOTAL NUM OF MOLS: \", num_mol)\n",
    "    return atom2num, num2atom, max_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! --- GET WEIGHTS ---\n",
    "nweights = {\n",
    "    'C':    0.03238897867833534,\n",
    "    'Br':   14.044943820224718,\n",
    "    'N':    0.21620219229022983,\n",
    "    'O':    0.2177273617975571,\n",
    "    'S':    1.6680567139282736,\n",
    "    'Cl':   2.872737719046251,\n",
    "    'F':    1.754693805930865,\n",
    "    'P':    37.735849056603776,\n",
    "    'I':    100.0,\n",
    "    'B':    416.6666666666667,\n",
    "    'Si':   454.54545454545456,\n",
    "    'Se':   833.3333333333334\n",
    "}\n",
    "bweights = { \n",
    "    BT.SINGLE:      4.663287337775892, \n",
    "    BT.AROMATIC:    4.77780803722868, \n",
    "    BT.DOUBLE:      34.74514436607484, \n",
    "    BT.TRIPLE:      969.9321047526673 \n",
    "}\n",
    "\n",
    "nweights_list = [nweights[k] for k in atom2num]\n",
    "bweights_list = [bweights[k] for k in bond2num]\n",
    "bweights_list.insert(0, 1500)\n",
    "node_weights = torch.tensor(nweights_list) \n",
    "edge_weights = torch.tensor(bweights_list) \n",
    "#!-------------------------------------------\n",
    "\n",
    "#! --- SET UP EXPERIMENT ---\n",
    "LRrnn, LRout = 1e-2, 1e-2\n",
    "# wd = 5e-4\n",
    "epoch, max_epoch = 1, 5001\n",
    "device, cuda, train_log, val_log = setup()\n",
    "train_dataset_loader, val_dataset_loader = create_train_val_dataloaders(train_data, valid_data, max_num_node, max_prev_node) #! HERE WORKERS\n",
    "rnn, output = get_generator()\n",
    "optimizer_rnn = torch.optim.RMSprop(list(rnn.parameters()), lr=LRrnn) # , eps=1e-5)  # , weight_decay=wd)\n",
    "optimizer_output = torch.optim.RMSprop(list(output.parameters()), lr=LRout) #, eps=1e-5)  # , weight_decay=wd)\n",
    "\n",
    "scheduler_rnn = torch.optim.lr_scheduler.OneCycleLR(optimizer_rnn, max_lr=LRrnn, steps_per_epoch=len(train_dataset_loader), epochs=max_epoch)\n",
    "scheduler_output = torch.optim.lr_scheduler.OneCycleLR(optimizer_output, max_lr=LRout, steps_per_epoch=len(train_dataset_loader), epochs=max_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_means_rnn = {}\n",
    "layer_stds_rnn = {}\n",
    "layer_hists = {}\n",
    "activations = {}\n",
    "grads = {}\n",
    "for idx, (name, module) in enumerate(rnn.named_children()):\n",
    "    if isinstance(module, nn.GRU):\n",
    "        for ii in range(0,2):\n",
    "            layer_means_rnn[f'{name}_output_{idx}'] = []\n",
    "            layer_means_rnn[f'{name}_hidden_{idx}'] = []\n",
    "            layer_stds_rnn[f'{name}_output_{idx}'] = []\n",
    "            layer_stds_rnn[f'{name}_hidden_{idx}'] = []\n",
    "            layer_hists[f'{name}_output_{idx}'] = []\n",
    "            layer_hists[f'{name}_hidden_{idx}'] = []\n",
    "            activations[f'{name}_output_{idx}'] = []\n",
    "            activations[f'{name}_hidden_{idx}'] = []            \n",
    "    else:\n",
    "        layer_means_rnn[name] = []\n",
    "        layer_stds_rnn[name] = []\n",
    "        layer_hists[name] = []\n",
    "        activations[name] = []\n",
    "        grads[name] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_means_rnn, layer_stds_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_append_(idx, name, module, input, output):\n",
    "    if isinstance(module, nn.GRU):        \n",
    "        pass\n",
    "    else:    \n",
    "        grads[name].append(output[0].grad.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_(idx, name, module, input, output):\n",
    "    if isinstance(module, nn.GRU):        \n",
    "        layer_means_rnn[f'{name}_output_{idx}'].append(output[0].detach().cpu().numpy().mean())\n",
    "        layer_stds_rnn[f'{name}_output_{idx}'].append(output[0].detach().cpu().numpy().std())\n",
    "        layer_means_rnn[f'{name}_hidden_{idx}'].append(output[1].detach().cpu().numpy().mean())\n",
    "        layer_stds_rnn[f'{name}_hidden_{idx}'].append(output[1].detach().cpu().numpy().std())\n",
    "        layer_hists[f'{name}_output_{idx}'].append(output[0].detach().cpu().abs().histc(40,0,10))\n",
    "        layer_hists[f'{name}_hidden_{idx}'].append(output[1].detach().cpu().abs().histc(40,0,10))\n",
    "    else:\n",
    "        layer_means_rnn[name].append(output.detach().cpu().numpy().mean())\n",
    "        layer_stds_rnn[name].append(output.detach().cpu().numpy().std())\n",
    "        layer_hists[name].append(output.detach().cpu().abs().histc(40,0,10))\n",
    "        activations[name].append(output.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (module_name, module) in enumerate(rnn.named_children()):\n",
    "    module.register_forward_hook(partial(append_, idx, module_name))\n",
    "    # module.register_backward_hook(partial(backward_append_, idx, module_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients={}\n",
    "for name, param in rnn.named_parameters():\n",
    "    gradients[name] = []\n",
    "\n",
    "# for name, param in output.named_parameters():\n",
    "#     gradients[name] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "update = []\n",
    "\n",
    "while epoch <= 75:\n",
    "    loss_this_epoch, loss_edg, loss_nodes = train_rnn_epoch(rnn=rnn, output=output,\n",
    "                                                            data_loader_=train_dataset_loader,\n",
    "                                                            optimizer_rnn=optimizer_rnn, optimizer_output=optimizer_output,\n",
    "                                                            node_weights=node_weights, edge_weights=edge_weights)\n",
    "    for name, param in rnn.named_parameters():\n",
    "        gradients[name].append(param.grad.detach().cpu().numpy())\n",
    "\n",
    "    update.append([(optimizer_rnn.param_groups[0]['lr']* p.grad.std()/p.data.std()).log10().item() for p in rnn.parameters()])\n",
    "\n",
    "    epoch +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, k in enumerate(layer_means_rnn.keys()):\n",
    "    plt.plot([i for i in range(len(layer_means_rnn[k]))], layer_means_rnn[k], label=f'{k} mean'.format(i=color))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, k in enumerate(layer_means_rnn.keys()):\n",
    "    plt.plot([i for i in range(len(layer_stds_rnn[k]))], layer_stds_rnn[k], label=f'{k} std'.format(i=color))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def get_hist(h): \n",
    "    return torch.stack(h).t().float().log1p() # with log you can see the range more clearly\n",
    "\n",
    "fig = plt.figure(figsize=(20., 20.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 5),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.5,  # pad between axes in inch.\n",
    "                )\n",
    "\n",
    "for ax, k in zip(grid, list(layer_hists.keys())):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    im = get_hist(layer_hists[k])\n",
    "    ax.imshow(im,  origin=\"lower\",)\n",
    "    ax.set_title(k)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(h):\n",
    "    h1 = torch.stack(h).t().float()\n",
    "    return h1[0]/h1.sum(0)\n",
    "\n",
    "fig, axes = plt.subplots(5,2, figsize=(15, 6))\n",
    "fig.tight_layout()\n",
    "for ax, k in zip(axes.flatten(), list(layer_hists.keys())):\n",
    "    h = layer_hists[k]\n",
    "    ax.plot(get_min(h))\n",
    "    ax.set_title(k)\n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legends = []\n",
    "for k in activations.keys():\n",
    "    hy, hx = np.histogram(activations[k], density = True)\n",
    "    plt.plot(hx[:-1], hy)\n",
    "    legends.append(k)\n",
    "\n",
    "plt.legend(legends)\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5,2, figsize=(15, 6))\n",
    "fig.tight_layout()\n",
    "for ax, k in zip(axes.flatten(), list(activations.keys())):\n",
    "    hy, hx = np.histogram(activations[k], density = True)\n",
    "    ax.plot(hx[:-1], hy)\n",
    "    ax.set_title(k)\n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([update[j] for j in range(len(update))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum()/len(gradients['embedding.weight'])\n",
    "x =np.zeros_like(gradients['embedding.weight'][0])\n",
    "for i in gradients['embedding.weight']:\n",
    "    x+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphRNN_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
